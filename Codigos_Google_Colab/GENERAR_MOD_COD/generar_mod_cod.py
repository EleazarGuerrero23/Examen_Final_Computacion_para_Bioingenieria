# -*- coding: utf-8 -*-
"""GENERAR_MOD_COD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pdh1yIoHz1d-PSnEE9h_Ze9nfIrxFSi2
"""

#!pip install rdkit pandas scikit-learn torch tqdm selfies

#from google.colab import files
#uploaded = files.upload()

import pandas as pd

# Cargar el archivo
df = pd.read_csv("derivados_antibioticos_sinteticos.csv")

# Crear la columna 'affinity_label' como el promedio de las dos afinidades
df["affinity_label"] = df[["affinity_ndm1", "affinity_oxa48"]].mean(axis=1)

# Verificamos que ahora estén las columnas correctas
assert "smiles" in df.columns and "affinity_label" in df.columns

# Mostramos las primeras filas
print(df[["smiles", "affinity_ndm1", "affinity_oxa48", "affinity_label"]].head())

all_smiles = df["smiles"].values
charset = sorted(set("".join(all_smiles)))
char_to_idx = {c: i + 1 for i, c in enumerate(charset)}  # Reservamos 0 para padding
idx_to_char = {i: c for c, i in char_to_idx.items()}
vocab_size = len(char_to_idx) + 1  # +1 por padding
max_len = max(len(s) for s in all_smiles)

from torch.utils.data import Dataset

class ConditionalSmilesDataset(Dataset):
    def __init__(self, smiles, labels):
        self.smiles = smiles
        self.labels = labels

    def __len__(self):
        return len(self.smiles)

    def __getitem__(self, idx):
        smi = self.smiles[idx]
        label = self.labels[idx]
        x = [char_to_idx[c] for c in smi]
        x = x + [0] * (max_len - len(x))  # padding
        return torch.tensor(x[:-1]), torch.tensor(x[1:]), torch.tensor(label, dtype=torch.float32)

from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader

X_train, X_val, y_train, y_val = train_test_split(df["smiles"], df["affinity_label"], test_size=0.1, random_state=42)
train_dataset = ConditionalSmilesDataset(X_train.values, y_train.values)
val_dataset = ConditionalSmilesDataset(X_val.values, y_val.values)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64)

import torch
import torch.nn as nn

class ConditionalVAE(nn.Module):
    def __init__(self, vocab_size, hidden_dim=256, latent_dim=128, cond_dim=1):
        super().__init__()
        self.latent_dim = latent_dim  # <-- AÑADIDO AQUÍ
        self.embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)
        self.encoder = nn.GRU(hidden_dim, hidden_dim, batch_first=True)
        self.hidden_to_mu = nn.Linear(hidden_dim, latent_dim)
        self.hidden_to_logvar = nn.Linear(hidden_dim, latent_dim)
        self.latent_to_hidden = nn.Linear(latent_dim + cond_dim, hidden_dim)
        self.decoder = nn.GRU(hidden_dim, hidden_dim, batch_first=True)
        self.out = nn.Linear(hidden_dim, vocab_size)

    def encode(self, x, c):
        embedded = self.embedding(x)
        _, h = self.encoder(embedded)
        h = h.squeeze(0)
        mu = self.hidden_to_mu(h)
        logvar = self.hidden_to_logvar(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z_cond, seq_len):
        h0 = self.latent_to_hidden(z_cond).unsqueeze(0)
        inputs = torch.zeros((z_cond.size(0), seq_len, self.embedding.embedding_dim)).to(z_cond.device)
        outputs, _ = self.decoder(inputs, h0)
        return self.out(outputs)

    def forward(self, x, c):
        mu, logvar = self.encode(x, c)
        z = self.reparameterize(mu, logvar)
        c = c.unsqueeze(1).float()  # Convierte [batch_size] → [batch_size, 1]
        z_cond = torch.cat([z, c], dim=1)
        recon = self.decode(z_cond, x.size(1))
        return recon, mu, logvar

import torch.nn.functional as F

def vae_loss(recon_logits, target, mu, logvar):
    # Reconstrucción: usamos cross entropy ignorando padding (índice 0)
    recon_loss = F.cross_entropy(
        recon_logits.view(-1, recon_logits.size(-1)),
        target.view(-1),
        ignore_index=0
    )
    # KL Divergence
    kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss + kl_loss, recon_loss.item(), kl_loss.item()

import torch
import time

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = ConditionalVAE(vocab_size).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

n_epochs = 10000

for epoch in range(n_epochs):
    model.train()
    total_loss, total_recon, total_kl = 0, 0, 0

    for x, y, labels in train_loader:
        x, y, labels = x.to(device), y.to(device), labels.to(device)
        optimizer.zero_grad()
        logits, mu, logvar = model(x, labels)
        loss, recon, kl = vae_loss(logits, y, mu, logvar)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        total_recon += recon
        total_kl += kl

    print(f"Epoch {epoch+1}/{n_epochs} | Loss: {total_loss:.2f} | Recon: {total_recon:.2f} | KL: {total_kl:.2f}")

import torch.nn.functional as F

def sample_from_model(model, n_samples=20, label_value=0):
    model.eval()
    sampled_smiles = []

    with torch.no_grad():
        for _ in range(n_samples):
            # Crear etiqueta condicional (tensor shape: [1, 1])
            label = torch.tensor([[label_value]]).float().to(device)

            # Muestreo aleatorio del espacio latente
            z = torch.randn(1, model.latent_dim).to(device)
            z_cond = torch.cat([z, label], dim=1)  # Concatenar z con etiqueta

            # Decodificación
            logits = model.decode(z_cond, max_len)
            probs = F.softmax(logits, dim=-1)
            sampled_indices = torch.argmax(probs, dim=-1).squeeze().cpu().numpy()

            # Reconstruir SMILES
            smiles = ''.join([idx_to_char.get(i, '') for i in sampled_indices])
            sampled_smiles.append(smiles)

    return sampled_smiles

# Generamos 20 nuevos derivados resistentes
nuevos_smiles = sample_from_model(model, n_samples=20, label_value=0)

print("SMILES generados:")
for smi in nuevos_smiles:
    print(smi)

from rdkit import Chem

generated_smiles = nuevos_smiles

def validar_smiles(smiles_list):
    resultados = []
    for smi in smiles_list:
        mol = Chem.MolFromSmiles(smi)
        if mol:
            resultados.append((smi, True))
        else:
            resultados.append((smi, False))
    return resultados

# Supón que tus 20 SMILES generados están en la lista 'generated_smiles'
resultados = validar_smiles(generated_smiles)

for smi, valido in resultados:
    print(f"{'✅' if valido else '❌'} {smi}")